import os
import io
import tempfile
import base64
import re  
from pathlib import Path
from typing import List, Optional

import numpy as np
import streamlit as st
import networkx as nx
from PIL import Image

from src.config import settings
from src.video.preprocess import extract_frames
from src.features.clip_extractor import CLIPFeatureExtractor
from src.features.captioner import generate_captions
from src.tree.builder import Leaf, build_adaptive_tree, save_tree

from src.utils.tree_export import to_nested_tree
from src.utils.d3_viewer import build_d3_html
from streamlit.components.v1 import html

from src.llm.query import rank_leaves_by_query, gather_context_from_topk, openai_answer


st.set_page_config(page_title="Adaptive Video Tree for LLM", layout="wide")

st.markdown("""
<style>
.block-container { padding-top: 1.2rem; padding-bottom: 3.6rem; }
h1, h2, h3 { letter-spacing: .2px; }
.kbd { background:#f5f5f7;border-radius:6px;padding:2px 6px;border:1px solid #e5e7eb;font-family:ui-monospace,SFMono-Regular,Menlo,Monaco;}
.small-note { color:#64748b;font-size:0.9rem; }
</style>
""", unsafe_allow_html=True)

st.title("üéÑ Adaptive Tree-based Video Representation for LLM")


def _ensure_defaults():
    ss = st.session_state
    ss.setdefault("pending_video_path", "")  
    ss.setdefault("video_path", "")         
    ss.setdefault("video_fp", "")
    ss.setdefault("frames", None)      
    ss.setdefault("embs", None)         
    ss.setdefault("captions", None)     
    ss.setdefault("G", None)            
    ss.setdefault("messages", [])       
    ss.setdefault("last_rank", None)    
    ss.setdefault("processing", False)  

def _file_fp(p: str) -> str:
    try:
        stat = Path(p).stat()
        return f"{stat.st_size}-{int(stat.st_mtime)}"
    except Exception:
        return Path(p).name

def _reset_video_state(keep_chat: bool = True):
    keys = ["frames", "embs", "captions", "G", "last_rank"]
    for k in keys:
        st.session_state.pop(k, None)
    if not keep_chat:
        st.session_state["messages"] = []

_ensure_defaults()


with st.expander("‚öôÔ∏è Settings (technical)", expanded=False):
    target_fps = st.number_input("Sampling FPS", value=settings.TARGET_FPS, step=0.1)
    sim_thr = st.slider("Merge similarity threshold", 0.70, 0.99,
                        value=settings.SIMILARITY_THRESHOLD, step=0.01)
    max_frames = st.number_input("Max frames (0 = unlimited)", value=0)
    captioning = st.checkbox("Generate offline captions (BLIP)",
                             value=settings.CAPTIONING_ENABLED)
    topk_ctx = st.slider("Top-K frames for context", 1, 20, 5)


@st.cache_resource(show_spinner=False)
def load_clip() -> CLIPFeatureExtractor:
    return CLIPFeatureExtractor()

@st.cache_data(show_spinner=True)
def compute_embeddings(image_paths: list[str]) -> np.ndarray:
    clip = load_clip()
    return clip.encode_images(image_paths)

@st.cache_data(show_spinner=False)
def _thumb_data_url(src_path: str, size=(224, 126), quality=90) -> str:
    """T·∫°o thumbnail s·∫Øc n√©t v√† tr·∫£ v·ªÅ data URL (base64)."""
    img = Image.open(src_path).convert("RGB")
    try:
        resample = Image.Resampling.LANCZOS
    except Exception:
        resample = Image.LANCZOS
    img.thumbnail(size, resample)
    buf = io.BytesIO()
    img.save(buf, format="WEBP", quality=quality, method=6)
    b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    return f"data:image/webp;base64,{b64}"

def _vi_translate_captions(caps: list[str]) -> list[str]:
    """D·ªãch caption sang ti·∫øng Vi·ªát (n·∫øu c√≥ API key). Lu√¥n ch·∫°y ƒë·ªÉ c√¢y hi·ªÉn th·ªã VI."""
    if not settings.OPENAI_API_KEY or not any(caps):
        return caps
    numbered = "\n".join(f"{i+1}. {c}" for i, c in enumerate(caps))
    prompt = (
        "B·∫°n l√† m·ªôt tr·ª£ l√Ω d·ªãch thu·∫≠t. H√£y d·ªãch c√°c caption sau sang TI·∫æNG VI·ªÜT, "
        "gi·ªØ nguy√™n th·ª© t·ª±, ng·∫Øn g·ªçn, t·ª± nhi√™n. Ch·ªâ tr·∫£ v·ªÅ danh s√°ch theo s·ªë th·ª© t·ª±.\n\n"
        + numbered
    )
    out = openai_answer(prompt, "") or ""
    lines = [l.strip() for l in out.splitlines() if l.strip()]
    cleaned = []
    for l in lines:
        stripped = l
        for sep in [". ", ") ", ".", ")", "- "]:
            head = stripped[:4]
            if any(head.startswith(f"{d}{sep}") for d in "123456789"):
                idx = stripped.find(sep)
                if idx != -1 and idx <= 3:
                    stripped = stripped[idx+len(sep):].strip()
                break
        cleaned.append(stripped)
    return cleaned if len(cleaned) == len(caps) else caps

def _compress_context(ctx: str, max_chars: int = 3500) -> str:
    """C·∫Øt ng·∫Øn ng·ªØ c·∫£nh ƒë·ªÉ LLM ph·∫£n h·ªìi nhanh h∆°n (kh√¥ng ƒë·ªïi logic)."""
    if len(ctx) <= max_chars:
        return ctx
    head = ctx[: max_chars - 200]
    tail = ctx[-200:]
    return head + "\n...\n" + tail


def _leaf_index_from_node(node_id, data) -> Optional[int]:
    """L·∫•y ch·ªâ s·ªë leaf t·ª´ thu·ªôc t√≠nh 'idx' ho·∫∑c t√™n node 'L{n}'."""
    if "idx" in data and isinstance(data["idx"], (int, np.integer)):
        return int(data["idx"])
    m = re.match(r"^L(\d+)$", str(node_id))
    return int(m.group(1)) if m else None

def _force_vi_captions_on_graph(G: nx.DiGraph, captions_vi: list[str]) -> int:
    """
    G√°n l·∫°i caption/label/title (v√† c√°c alias th∆∞·ªùng g·∫∑p) cho to√†n b·ªô leaf = ti·∫øng Vi·ªát.
    Tr·∫£ v·ªÅ s·ªë node leaf ƒë√£ c·∫≠p nh·∫≠t.
    """
    changed = 0
    for n, d in G.nodes(data=True):
        if d.get("kind") == "leaf":
            li = _leaf_index_from_node(n, d)
            if li is not None and 0 <= li < len(captions_vi):
                cap_vi = captions_vi[li] or d.get("caption", "")
                for key in ("caption", "label", "title", "text", "name", "desc", "description"):
                    G.nodes[n][key] = cap_vi
                changed += 1
    return changed

tab_video, tab_tree, tab_qa, tab_sum = st.tabs(
    ["üé• Video", "üå≥ C√¢y video", "üí¨ H·ªèi ‚Äì ƒë√°p", "üìù T√≥m t·∫Øt"]
)

with tab_video:
    st.markdown("**T·∫£i video** (ch·∫•p nh·∫≠n: mp4, mov, mkv, avi)")
    uploaded = st.file_uploader("Ch·ªçn video", type=["mp4", "mov", "mkv", "avi"], label_visibility="collapsed")

    preview_path = ""
    if uploaded is not None:
        suffix = Path(uploaded.name).suffix or ".mp4"
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(uploaded.getbuffer())
            st.session_state["pending_video_path"] = tmp.name
        preview_path = st.session_state["pending_video_path"]
    elif st.session_state.get("video_path"):
        preview_path = st.session_state["video_path"]

    if preview_path:
        st.video(preview_path)

    col1, col2 = st.columns([1, 4])
    with col1:
        start = st.button("üöÄ N·∫°p video")
    with col2:
        if not st.session_state.get("pending_video_path") and not st.session_state.get("video_path"):
            st.caption("H√£y ch·ªçn m·ªôt video r·ªìi b·∫•m **N·∫°p video** ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        else:
            st.caption("B·∫•m **N·∫°p video** ƒë·ªÉ x·ª≠ l√Ω.")

    if start:
        if st.session_state.get("pending_video_path"):
            new_path = st.session_state["pending_video_path"]
            fp = _file_fp(new_path)
            if fp != st.session_state.get("video_fp"):
                _reset_video_state(keep_chat=True)
                st.session_state["video_path"] = new_path
                st.session_state["video_fp"] = fp
            else:
                st.session_state["video_path"] = new_path
        elif st.session_state.get("video_path"):
            pass
        else:
            st.warning("Vui l√≤ng ch·ªçn video tr∆∞·ªõc khi n·∫°p.")
            st.stop()

        st.session_state["processing"] = True

    if st.session_state.get("processing"):
        video_path = st.session_state["video_path"]

        out_dir = os.path.join(
            settings.DATA_DIR,
            settings.FRAMES_DIRNAME,
            f"{Path(video_path).stem}_{st.session_state['video_fp'].replace(':','-')}"
        )
        os.makedirs(out_dir, exist_ok=True)

        st.info("üîß ƒêang tr√≠ch xu·∫•t khung h√¨nh‚Ä¶")
        frames = extract_frames(
            video_path,
            out_dir=out_dir,
            target_fps=target_fps,
            max_frames=None if max_frames == 0 else int(max_frames),
            frame_size=settings.FRAME_SIZE,
        )
        st.success(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t {len(frames)} khung h√¨nh.")

        image_paths = [f.path for f in frames]

        st.info("üß† ƒêang t√≠nh embedding CLIP‚Ä¶")
        embs = compute_embeddings(image_paths)
        st.success("‚úÖ Ho√†n t·∫•t embedding.")

        if captioning:
            st.info("üìù ƒêang sinh m√¥ t·∫£ khung (captions)‚Ä¶")
            caps_en = generate_captions(image_paths)
            st.success("‚úÖ ƒê√£ t·∫°o captions (EN).")
            st.info("üåê ƒêang d·ªãch captions sang Ti·∫øng Vi·ªát‚Ä¶")
            captions = _vi_translate_captions(caps_en)
            st.success("‚úÖ ƒê√£ d·ªãch captions (VI).")
        else:
            captions = ["" for _ in image_paths]
            st.info("‚è≠Ô∏è B·ªè qua t·∫°o captions (ƒëang t·∫Øt).")

        st.session_state["frames"] = frames
        st.session_state["embs"] = embs
        st.session_state["captions"] = captions

        st.info("üå≤ ƒêang x√¢y d·ª±ng c√¢y‚Ä¶")
        leaves = [
            Leaf(idx=i, time_s=float(frames[i].time_s),
                 image_path=image_paths[i], emb=embs[i], caption=captions[i])
            for i in range(len(frames))
        ]
        G: nx.DiGraph = build_adaptive_tree(leaves, threshold=sim_thr)

        _force_vi_captions_on_graph(G, captions)

        st.session_state["G"] = G
        save_tree(G, os.path.join(settings.DATA_DIR, settings.TREE_JSON))
        st.success("‚úÖ Ho√†n t·∫•t x√¢y d·ª±ng c√¢y.")

        st.session_state["processing"] = False
        st.success("üéâ X·ª≠ l√Ω xong! Chuy·ªÉn sang tab **C√¢y video** ho·∫∑c **H·ªèi ‚Äì ƒë√°p** ƒë·ªÉ xem.")

with tab_tree:
    if st.session_state.get("G") is not None:
        tree_json = to_nested_tree(st.session_state["G"]) 
        viewer_html = build_d3_html(
            tree_json,
            width=1200,
            height=700,
            thumb_w=224,
            thumb_h=126
        )
        html(viewer_html, height=720, scrolling=True)

with tab_qa:
    if st.session_state.get("frames") is not None:
        st.markdown("**ƒê·∫∑t c√¢u h·ªèi v·ªÅ video** *(LLM tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát)*")

        for role, content in st.session_state["messages"]:
            with st.chat_message(role):
                st.markdown(content)

        q = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n‚Ä¶")
        if q:
            st.session_state["messages"].append(("user", q))

            ranking = rank_leaves_by_query(
                st.session_state["G"], q, load_clip(), st.session_state["embs"]
            )
            st.session_state["last_rank"] = ranking
            ctx_raw = gather_context_from_topk(st.session_state["G"], ranking, k=topk_ctx)

            ctx = _compress_context(ctx_raw, max_chars=3500)
            prompt_vi = f"H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng b·∫±ng **TI·∫æNG VI·ªÜT**.\n\nC√¢u h·ªèi: {q}"

            ans = openai_answer(prompt_vi, ctx) or "Xin l·ªói, m√¨nh ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi ph√π h·ª£p."
            st.session_state["messages"].append(("assistant", ans))

            with st.chat_message("assistant"):
                st.markdown(ans)

with tab_sum:
    if st.session_state.get("frames") is not None:
        st.markdown("**T√≥m t·∫Øt video (ƒëo·∫°n vƒÉn, ti·∫øng Vi·ªát)**")
        if st.button("T√≥m t·∫Øt video"):
            n = len(st.session_state["frames"])
            ones = list(enumerate(np.ones(n, dtype=float)))
            ctx_raw = gather_context_from_topk(st.session_state["G"], ones, k=topk_ctx)
            ctx = _compress_context(ctx_raw, max_chars=3500)

            prompt_sum = (
                "H√£y vi·∫øt m·ªôt ƒëo·∫°n vƒÉn t√≥m t·∫Øt ng·∫Øn g·ªçn b·∫±ng **TI·∫æNG VI·ªÜT** (kho·∫£ng 4‚Äì6 c√¢u). "
                "Gi·ªØ m·∫°ch k·ªÉ t·ª± nhi√™n, c√≥ k·∫øt n·ªëi gi·ªØa c√°c c√¢u, **kh√¥ng** d√πng g·∫°ch ƒë·∫ßu d√≤ng, "
                "tr√°nh l·∫∑p √Ω. "
                "T·∫≠p trung v√†o c√°c c·∫£nh/ƒëo·∫°n n·ªïi b·∫≠t v√† di·ªÖn ti·∫øn ch√≠nh."
            )

            summary = openai_answer(prompt_sum, ctx) or "Kh√¥ng t·∫°o ƒë∆∞·ª£c t√≥m t·∫Øt."
            st.markdown(summary)
